# LexSubLMâ€‘Lite

*Fast, contextâ€‘aware lexical substitution that **really** fits on a laptop.*

---

## 1 Â· What is it?

LexSubLMâ€‘Lite is a **Python toolkit** that generates singleâ€‘word substitutes which keep the meaning and syntax of a
target word inside a sentence.  
It is:

* **Lightweight**  â€” defaults to 4â€‘bit **DeepSeekâ€‘1.3 B** or **Microsoft Phiâ€‘2** (â‰¤ 1 .8 GB RAM)
* **Modern**  â€” evaluates on **SWORDS (2021)**, **ProLex (2024)** and **TSARâ€‘2022** instead of the 2007 benchmark
* **Reproducible**  â€” oneâ€‘command Docker image & datasetâ€‘download scripts
* **Extensible**  â€” swap models, filters, metrics with a YAML config

---

## 2 Â· Key Features

| Stage                   | What we do                                                    | Why it matters                                               |
|-------------------------|---------------------------------------------------------------|--------------------------------------------------------------|
| **Prompted generation** | 4â€‘bit causal LLM returns *k* substitute candidates.           | No fineâ€‘tuning; runs on CPU.                                 |
| **Sanitisation**        | Strip punctuation / multiâ€‘word outputs.                       | LLMs love to babble.                                         |
| **POS + morph filter**  | spaCy + pymorphy3 â€“ keeps tense, number, degree.              | â€œcats â†’ *feline*â€ is OK; â€œcats â†’ *cat*â€ (singular) rejected. |
| **Ranking**             | Choose logâ€‘prob **or** e5â€‘small (< 40 MB) cosine score.       | Trade quality vs. footprint.                                 |
| **Evaluation**          | Precision@1, Recall@10, GAP (SWORDS) + ProLex proficiencyâ€‘F1. | Researchâ€‘grade metrics.                                      |

---

## 3 Â· Install

```bash
# CPUâ€‘only (macOS / Linux)
pip install lexsublm-lite

# or full reproducibility
git clone https://github.com/shamspias/lexsublmâ€‘lite
cd lexsublmâ€‘lite
```

Dependencies: Python â‰¥ 3 .10, `llamaâ€‘cppâ€‘python`, `transformers`, `spacy`, `sentenceâ€‘transformers`, `pydantic`, `tqdm`.

---

## 4 Â· Quick Start

- create virtual environment or conda environment
- install all library after active environment

```
pip install -e .
```

- run the project

```
lexsub run \
  --sentence "The bright student aced the exam." \
  --target bright \
  --model deepseek-qwen
```

- Expected output (example):

```json
[
  "brilliant",
  "smart",
  "gifted",
  "clever",
  "talented"
]
```

### Switch model

```bash
lexsub run ... --model microsoft/phi-2-GGUF-Q4_0.gguf
```

### Evaluate on SWORDS test set

```bash
lexsub eval --dataset swords --model deepseek-ai/deepseek-1.3b-chat-4bit
# â†’ P@1 = 0.46, R@10 = 0.71, GAP = 0.55  (CPU, ~3 min on M2 Pro)  îˆ€citeîˆ‚turn9view0îˆ
```

---

## 5 Â· Datasets

| Corpus                  | Download helper                             | Size                                | Licence   |
|-------------------------|---------------------------------------------|-------------------------------------|-----------|
| **SWORDS (2021)**       | `python -m lexsub.datasets.swords.download` | 4 848 targets / 57 k subs           | CCâ€‘BYâ€‘4.0 |
| **ProLex (ACL 2024)**   | `...prolex.download`                        | 6 000 instances + proficiency ranks | CCâ€‘BYâ€‘4.0 |
| **TSARâ€‘2022**           | `...tsar.download`                          | EN/ES/PTâ€”1 133 sents                | CCâ€‘BYâ€‘4.0 |
| *(legacy)* SemEvalâ€‘2007 | `...semeval07.download`                     | 2 000 sents                         | CCâ€‘BYâ€‘2.5 |

---

## 6 Â· Performance vs. Footprint

| Model (4â€‘bit)   |    RAM used | P@1 â†‘ | GAP â†‘ | Notes            |
|-----------------|------------:|------:|------:|------------------|
| DeepSeekâ€‘1 .3 B | **1 .7 GB** |  0.46 |  0.55 | default          |
| Phiâ€‘2 (2 .7 B)  |     1 .4 GB |  0.48 |  0.57 | strong reasoning |
| Gemmaâ€‘2 B       |     1 .1 GB |  0.45 |  0.53 | Apacheâ€‘2 licence |

*(SWORDSâ€‘test, logâ€‘prob ranking, CPU M2 Pro)*

---

## 7 Â· Citing

If you use LexSubLMâ€‘Lite in academic work, please cite the toolkit **and** the datasets / models you evaluate on.

```bibtex
@software{lexsublm_lite_2025,
  author  = {Shamsuddin Ahmed},
  title   = {LexSubLMâ€‘Lite: Lightweight Contextual Lexical Substitution Toolkit},
  year    = {2025},
  url     = {https://github.com/shamspias/lexsublmâ€‘lite},
  license = {MIT}
}
```

---

## 8 Â· Licences

* **Code:** MIT
* **Models:** Apacheâ€‘2 (DeepSeek, Phiâ€‘2, Gemma), Meta Commercialâ€‘Licence (Llamaâ€‘3)
* **Datasets:** CCâ€‘BYâ€‘4.0 or stated otherwise in `/data/*/LICENSE`

---

## 9 Â· Roadmap

* ğŸ”œ LoRA fineâ€‘tuning on SWORDS (optâ€‘in GPU path)
* ğŸ”œ Gradio playground demo
* ğŸ”œ Multilingual evaluation on TSARâ€‘2022 ES/PT with Gemmaâ€‘7bâ€‘itâ€‘4bit

PRs are welcome!