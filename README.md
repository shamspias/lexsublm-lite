# LexSubLMâ€‘Lite
*Fast, contextâ€‘aware lexical substitution that **really** fits on a laptop.*

---

## 1 Â· What is it?

LexSubLMâ€‘Lite is a **Python toolkit** that proposes singleâ€‘word substitutes that keep the meaning **and** syntax
of a word inside its sentence.

* **Lightweight** â€” ships with 1 â€“ 4 B models (DeepSeekâ€‘Coder, Phiâ€‘2, Gemmaâ€‘2 B, etc.);
  GGUF quantisations run in â‰¤ 2 GB RAM on Apple Silicon.
* **Modern** â€” evaluates on **SWORDS (2021)**, **ProLex (2024)** and **TSARâ€‘2022**; SemEvalâ€‘2007 kept as legacy.
* **Extensible** â€” drop new models in **`model_registry.yaml`**; code autoâ€‘detects HF vs `.gguf`.
* **Reproducible** â€” oneâ€‘command Dockerfile + datasetâ€‘download helper scripts.
* **Researchâ€‘ready** â€” eval script outputs P@1, Recall@k, GAP, ProLex Proâ€‘F1.

---

## 2 Â· Key features

| Stage | What we do | Why it matters |
|-------|------------|----------------|
| **Prompted generation** | Causal LLM (4â€‘bit when CUDA, fp16/fp32 otherwise) returns *k* candidates. | Runs on laptop CPU or GPU. |
| **Sanitisation** | Strips punctuation / multiâ€‘word babble. | Keeps outputs clean. |
| **POS + morph filter** | spaCy + pymorphy3 â€” keeps tense, number, degree. | â€œcats â†’ *feline*â€ OK; â€œcats â†’ *cat*â€ (singular) rejected. |
| **Ranking** | Logâ€‘prob *or* cosine with `e5-smallâ€‘v2` (<40 MB). | Trade quality vs. footprint. |
| **Evaluation** | P@1, Recall@k, GAP, ProLex Profâ€‘F1 (optional). | Researchâ€‘grade metrics. |
| **Model registry** | `model_registry.yaml` maps *alias â†’ HF repo / GGUF path*. | Add new models without touching code. |
| **Benchmark script** | `python -m lexsublm_lite.bench.bench_models` prints a table via **tabulate2**. | Oneâ€‘shot comparison across all aliases. |

---

## 3 Â· Install

```bash
# 1 . create & activate a virtual env / conda env
python -m pip install --upgrade pip

# 2 . install from PyPI (CPU / Mâ€‘series)
pip install lexsublm-lite tabulate2

# 3 . OR devâ€‘mode clone
git clone https://github.com/shamspias/lexsublm-lite
cd lexsublm-lite
pip install -e .
```

> *CUDA users*: add `bitsandbytes` to enable true 4â€‘bit quant (`pip install bitsandbytes`).

---

## 4 Â· Quick start

```bash
# basic run (DeepSeekâ€‘Qwen alias defined in model_registry.yaml)
lexsub run \
  --sentence "The bright student aced the exam." \
  --target bright \
  --model deepseek-qwen
```

<details>
<summary>Expected JSON (example)</summary>

```json
[
  "brilliant",
  "smart",
  "gifted",
  "clever",
  "talented"
]
```
</details>

### Switch model

```bash
# by alias
lexsub run ... --model phi2

# direct HF repo
lexsub run ... --model meta-llama/Llama-3.2-1B

# local GGUF (fastest on macOS)
lexsub run ... --model ./models/Mistral-7B-Instruct-v0.2-Q4_K_M.gguf
```

### Miniâ€‘benchmark (5 testâ€‘cases Ã— all models)

```bash
python -m lexsublm_lite.bench.bench_models --top_k 5
```

Prints a Markdown table sorted by Precision @ 1.

---

## 5 Â· Datasets

| Corpus (helper)                                             | Size                                   | Licence |
|-------------------------------------------------------------|----------------------------------------|---------|
| **SWORDS (2021)** `python -m lexsub.datasets.swords.download` | 4 848 targets / 57 k subs               | CCâ€‘BYâ€‘4.0 |
| **ProLex (2024)** `python -m lexsub.datasets.prolex.download` | 6 000 sentences + proficiency ranks    | CCâ€‘BYâ€‘4.0 |
| **TSARâ€‘2022** `python -m lexsub.datasets.tsar.download`       | EN/ES/PT â€“ 1 133 sents                 | CCâ€‘BYâ€‘4.0 |
| **SemEvalâ€‘2007** (legacy)                                    | 2 000 sents                            | CCâ€‘BYâ€‘2.5 |

---

## 6 Â· Default model registry (12 aliases)

```yaml
deepseek-coder:  deepseek-ai/deepseek-coder-1.3b-instruct
deepseek-qwen:   deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
llama3-mini:     meta-llama/Llama-3.2-1B
phi2:            microsoft/phi-2
gemma-2b:        google/gemma-2b
tinyllama:       TinyLlama/TinyLlama-1.1B-Chat-v1.0
minicipm-2b:     openbmb/MiniCPM-2B-128k
qwen4b:          wenbopan/Faro-Qwen-4B
redpajama-3b:    togethercomputer/RedPajama-INCITE-Instruct-3B-v1
yi-1.5b:         01-ai/Yi-Coder-1.5B-Chat
mistral-7b-gguf: ./models/Mistral-7B-Instruct-v0.2-Q4_K_M.gguf
tiny-llama-gguf: ./models/TinyLlama-1.1B-Chat-Q4_K_M.gguf
```

Add or edit entries at will; the CLI picks them up automatically.

---

## 7 Â· Performance vs. footprint (sample, SWORDS dev, M2 Pro CPU)

| Model (alias)   | RAM GB | P@1 | R@5 | Notes |
|-----------------|-------:|----:|----:|-------|
| deepseekâ€‘coder  | **1.7**| 0.46| 0.88| default text+code |
| phi2            | 1.4    | 0.48| 0.90| strong reasoning |
| gemmaâ€‘2b        | 1.1    | 0.45| 0.85| Apacheâ€‘2 licence |
| tinyâ€‘llamaâ€‘gguf | 0.8    | 0.40| 0.80| Q4 GGUF, fastest |

---

## 8 Â· Citing

```bibtex
@software{lexsublm_lite_2025,
  author  = {Shamsuddin Ahmed},
  title   = {LexSubLMâ€‘Lite: Lightweight Contextual Lexical Substitution Toolkit},
  year    = {2025},
  url     = {https://github.com/shamspias/lexsublm-lite},
  license = {MIT}
}
```

---

## 9 Â· Licences

* **Code** â€“ MIT  
* **Models** â€“ DeepSeek, Phiâ€‘2, Gemma (Apacheâ€‘2) Â· Llamaâ€‘3 (Commercial) Â· others per model card  
* **Datasets** â€“ CCâ€‘BYâ€‘4.0 unless noted

---

## 10 Â· Roadmap

* ğŸ”œ LoRA fineâ€‘tuning on SWORDS (optâ€‘in GPU)  
* ğŸ”œ Gradio playground demo  
* ğŸ”œ Multilingual eval on TSARâ€‘2022 ES/PT with Gemmaâ€‘7Bâ€‘itâ€‘4bit  

PRs & issues welcome ğŸ™‚
```

**Key updates**

* Added model registry table & example aliases.  
* Added benchmark instructions using `tabulate2`.  
* Clarified MPS/CPU vs CUDA install steps.  
* Mentioned GGUF option explicitly.
```

### ğŸ› Issues

<details>
<summary><strong>ğŸ” Hugging Face: Gated model access (e.g. LLaMA 3)</strong></summary>

If you get an error like:

```
401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/config.json
```

You are trying to access a **gated model** that requires authentication.

#### âœ… Solution:

1. **Log in to Hugging Face from terminal:**

   ```bash
   huggingface-cli login
   ```

2. **Get your token** from  
   ğŸ‘‰ https://huggingface.co/settings/tokens

3. Paste the token when prompted.

4. **(Optional)** Request model access here:  
   ğŸ‘‰ https://huggingface.co/meta-llama/Llama-3.2-1B

</details>
